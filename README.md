# Parallel-Training-Strategies-for-Toy-Transformers-Sequential-Data-and-Pipeline
PyTorch implementation of a toy Transformer model with sequential, data-parallel, and pipeline-parallel (1F1B) training. Includes experiments analyzing memory consumption and throughput trade-offs across different hyperparameters
